\begin{abstract}
As Large Language Models (LLMs) become increasingly sophisticated, distinguishing machine-generated text from human writing has become a critical challenge. Existing benchmarks primarily focus on static datasets or specific model versions, failing to capture the dynamic ``arms race'' between generation and detection. We introduce the \textbf{Undetectability Leaderboard}, a dynamic evaluation framework with distinct tracks for Inference (base models) and Postediting (adversarial modifications). Using the RAID benchmark and a standard RoBERTa-based detector, we demonstrate that while state-of-the-art open-source models like LLaMA-Chat are moderately detectable (Undetectability Score $\approx 0.53$), simple adversarial attacks such as \textit{SpaceInfi} can nullify detection, achieving near-perfect undetectability scores ($>0.99$). Our results highlight the fragility of current token-based detectors and the necessity of a multi-track leaderboard to drive robust detection research.
\end{abstract}

